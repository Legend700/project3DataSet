ğŸ  House Price Prediction using Regression

---

### ğŸ“Œ Introduction

In this project, I worked on the Kaggle competition "House Prices: Advanced Regression Techniques." The goal is to predict the final sale price of homes based on various features such as size, location, number of rooms, and more. The dataset contains 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. The challenge is a regression problemâ€”where the target variable, `SalePrice`, is continuous.

Participants are provided with two datasets:
- `train.csv` â€“ contains 1,460 entries with both features and the sale price
- `test.csv` â€“ contains similar features but lacks the sale price column (for submissions)

While I had the option to submit predictions to Kaggle, I chose to split the training data into my own training and validation sets for model development and evaluation.

---

### ğŸ“ˆ What is Regression?

**Regression** is a type of supervised machine learning used to predict a continuous outcome variable based on one or more input features. It's useful in real-world scenarios like price prediction, stock forecasting, or health diagnostics.

#### ğŸ“ Linear Regression

Linear regression models the relationship between input features (independent variables) and a continuous target (dependent variable) by fitting a straight line. The equation is:

\[
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon
\]

Where:
- \( \hat{y} \) is the predicted target (SalePrice)
- \( x_i \) are input features (e.g., `GrLivArea`, `YearBuilt`)
- \( \beta_i \) are the model coefficients
- \( \epsilon \) is the error term

The goal of training is to minimize the error, often done using **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)**.


### ğŸ”¬ Experiment 1: Baseline Model

#### ğŸ§  Data Understanding

To start, I explored the data with:

- `df.describe()` to understand distributions
- `df.info()` to check data types and nulls
- `df.isnull().sum()` to inspect missing values
- Correlation heatmap to detect relationships

Strong correlations with `SalePrice` included:
- `OverallQual` (0.79)
- `GrLivArea` (0.70)
- `GarageCars`, `TotalBsmtSF`, and `YearBuilt`

#### ğŸ§¼ Pre-processing

- Selected top correlated features for simplicity: `OverallQual`, `GrLivArea`, `GarageCars`, `TotalBsmtSF`, `FullBath`, `YearBuilt`
- Handled missing values (dropped rows or filled with median)
- Standardized features with `StandardScaler`
- Split data: 80% training, 20% test

#### ğŸ§  Modeling

Used **LinearRegression** from `scikit-learn`:
```python
model = LinearRegression()
model.fit(X_train_scaled, y_train)
```

#### ğŸ“Š Evaluation

Predicted `SalePrice` and calculated RMSE:
```python
rmse = mean_squared_error(y_test, y_pred, squared=False)
```

**RMSE (Linear):** *e.g., 35,000*



### ğŸ§ª Experiment 2: Ridge Regression + Categorical Features

#### Changes:
- Added categorical variables: `Neighborhood`, `BldgType`
- Used `OneHotEncoder` for encoding
- Switched to **Ridge Regression** (regularized linear model)

#### Evaluation:
**RMSE (Ridge):** *e.g., 32,500*  
Improved slightly, likely due to richer information from encoded features and reduced overfitting.


### ğŸ§ª Experiment 3: Feature Engineering + Lasso Regression

#### Changes:
- Created new feature: `TotalSF` = `1stFlrSF + 2ndFlrSF + TotalBsmtSF`
- Used `Lasso Regression` to shrink unnecessary features

#### Evaluation:
**RMSE (Lasso):** *e.g., 31,800*  
Improved performance. Lassoâ€™s feature selection helped reduce noise from irrelevant inputs.



### ğŸ’­ Impact Section

Predicting house prices can greatly assist real estate agents, buyers, and urban planners. However, care must be taken:

- **Ethical**: Data bias can impact predictions across neighborhoods with different socioeconomic backgrounds.
- **Social**: Automated pricing can influence housing affordability or exacerbate inequality.
- **Privacy**: Real estate data must ensure individual property privacy.



### ğŸ§  Conclusion

This project taught me how small steps in preprocessing or feature selection can significantly affect regression model performance. I learned:

- **Feature engineering matters** (e.g., `TotalSF`)
- **Categorical variables are powerful** when properly encoded
- **Regularization techniques** like Ridge and Lasso often outperform basic linear models

Most importantly, it helped me appreciate the experimentation process that underlies building any good model.



### ğŸ”— References

- [Kaggle House Prices Competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
- [Scikit-learn LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [Ridge and Lasso Docs](https://scikit-learn.org/stable/modules/linear_model.html)
- Class lecture notes on regression modeling



### ğŸ§¾ Code

Full code available in this Jupyter notebook:  
ğŸ“ [Download house_price_regression_project.ipynb](sandbox:/mnt/data/house_price_regression_project.ipynb)

